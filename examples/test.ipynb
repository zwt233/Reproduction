{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "import random\n",
    "from scipy.sparse import csgraph\n",
    "import sys\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "from utils import *\n",
    "from graphConvolution import *\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)\n",
    "\n",
    "def aug_normalized_adjacency(adj):\n",
    "   adj = adj + sp.eye(adj.shape[0])\n",
    "   adj = sp.coo_matrix(adj)\n",
    "   row_sum = np.array(adj.sum(1))\n",
    "   d_inv_sqrt = np.power(row_sum, -0.5).flatten()\n",
    "   d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "   d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
    "   return d_mat_inv_sqrt.dot(adj).dot(d_mat_inv_sqrt).tocoo()\n",
    "\n",
    "\n",
    "def aug_random_walk(adj):\n",
    "   adj = adj + sp.eye(adj.shape[0])\n",
    "   adj = sp.coo_matrix(adj)\n",
    "   row_sum = np.array(adj.sum(1))\n",
    "   d_inv = np.power(row_sum, -1.0).flatten()\n",
    "   d_mat = sp.diags(d_inv)\n",
    "   return (d_mat.dot(adj)).tocoo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 1]: Upload pubmed dataset.\n",
      "| # of nodes : 19717\n",
      "| # of edges : 44325.5\n",
      "| # of features : 500\n",
      "| # of clases   : 3\n",
      "| # of train set : 60\n",
      "| # of val set   : 500\n",
      "| # of test set  : 1000\n"
     ]
    }
   ],
   "source": [
    "adj, features, labels, idx_train, idx_val, idx_test = load_data(dataset=\"pubmed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj = aug_normalized_adjacency(adj)\n",
    "adj = sparse_mx_to_torch_sparse_tensor(adj).float().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 50 # steps\n",
    "epoches = 200\n",
    "num_node =features.shape[0]\n",
    "feature_list = [features.cuda()]\n",
    "for i in range(k):\n",
    "    propagated_fea = torch.spmm(adj,feature_list[-1]).cuda()\n",
    "    feature_list.append(propagated_fea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = labels.cuda()\n",
    "idx_train = idx_train.cuda()\n",
    "idx_val = idx_val.cuda()\n",
    "idx_test = idx_test.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GMLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(epoch, model,record,feature_list):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output_att, output_concat = model(feature_list)\n",
    "    L1 = F.nll_loss(output_att[idx_train], labels[idx_train])\n",
    "    L2 = F.nll_loss(output_concat[idx_train], labels[idx_train])\n",
    "    loss_train = L1 + np.cos(np.pi*epoch/(2*epoches))*L2\n",
    "    acc_train = accuracy(output_att[idx_train], labels[idx_train])\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "    model.eval()\n",
    "    output_att, output_concat = model(feature_list)\n",
    "\n",
    "    loss_val = F.nll_loss(output_att[idx_val], labels[idx_val])\n",
    "    acc_val = accuracy(output_att[idx_val], labels[idx_val])\n",
    "    \n",
    "    loss_test = F.nll_loss(output_att[idx_test], labels[idx_test])\n",
    "    acc_test = accuracy(output_att[idx_test], labels[idx_test])\n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "          'acc_test: {:.4f}'.format(acc_test.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))\n",
    "    record[acc_val.item()] = acc_test.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class GMLP(nn.Module):\n",
    "    def __init__(self, nfeat,nhid, nclass, dropout):\n",
    "        super(GMLP, self).__init__()\n",
    "        \n",
    "        self.lr_left1 = nn.Linear((k+1)*features.shape[1], nhid)\n",
    "        self.lr_left2 = nn.Linear(nhid, nclass)\n",
    "        self.lr_att = nn.Linear(nhid+nfeat, 1)\n",
    "\n",
    "        self.lr_right1 = nn.Linear(nfeat, nhid)\n",
    "        self.lr_right2 = nn.Linear(nhid, nclass)\n",
    "        \n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self,feature_list):     \n",
    "        drop_features = [F.dropout(feature, self.dropout , training=self.training) for feature in feature_list]\n",
    "        concat_features = torch.cat(drop_features,dim=1)\n",
    "\n",
    "        left_1 = F.relu(self.lr_left1(concat_features))\n",
    "        left_1 = F.dropout(left_1, self.dropout, training=self.training)\n",
    "        left_2 = self.lr_left2(left_1)\n",
    "        \n",
    "        attention_scores = [torch.sigmoid(self.lr_att(torch.cat((left_1,x), dim=1))).view(num_node,1) for x in drop_features]\n",
    "        W = torch.cat(attention_scores, dim=1)\n",
    "        W = F.softmax(W,1)\n",
    "        \n",
    "        right_1 = torch.mul(drop_features[0], W[:,0].view(num_node,1)) \n",
    "        for i in range(1,k):\n",
    "            right_1 = right_1 + torch.mul(drop_features[i], W[:,i].view(num_node,1)) \n",
    "            \n",
    "        right_1 = F.relu(self.lr_right1(right_1))\n",
    "        right_1 = F.dropout(right_1, self.dropout, training=self.training)\n",
    "        \n",
    "        right_2 = self.lr_right2(right_1)\n",
    "        \n",
    "        return F.log_softmax(right_2, dim=1),F.log_softmax(left_2,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 2.1986 acc_train: 0.3333 acc_val: 0.4160 acc_test: 0.4070 time: 0.4837s\n",
      "Epoch: 0002 loss_train: 2.0278 acc_train: 0.3667 acc_val: 0.1960 acc_test: 0.1800 time: 0.1957s\n",
      "Epoch: 0003 loss_train: 10.0510 acc_train: 0.3500 acc_val: 0.4400 acc_test: 0.4600 time: 0.1956s\n",
      "Epoch: 0004 loss_train: 4.4637 acc_train: 0.4000 acc_val: 0.4160 acc_test: 0.4070 time: 0.1975s\n",
      "Epoch: 0005 loss_train: 7.9347 acc_train: 0.3333 acc_val: 0.4820 acc_test: 0.4630 time: 0.1981s\n",
      "Epoch: 0006 loss_train: 3.3514 acc_train: 0.4500 acc_val: 0.4500 acc_test: 0.4710 time: 0.1966s\n",
      "Epoch: 0007 loss_train: 2.1732 acc_train: 0.4667 acc_val: 0.5380 acc_test: 0.5350 time: 0.1968s\n",
      "Epoch: 0008 loss_train: 1.3744 acc_train: 0.5667 acc_val: 0.5740 acc_test: 0.5820 time: 0.1976s\n",
      "Epoch: 0009 loss_train: 1.6961 acc_train: 0.6500 acc_val: 0.6880 acc_test: 0.6730 time: 0.1963s\n",
      "Epoch: 0010 loss_train: 1.4667 acc_train: 0.7167 acc_val: 0.6460 acc_test: 0.6430 time: 0.1957s\n",
      "Epoch: 0011 loss_train: 2.7470 acc_train: 0.6500 acc_val: 0.6820 acc_test: 0.6640 time: 0.1967s\n",
      "Epoch: 0012 loss_train: 2.0089 acc_train: 0.7833 acc_val: 0.7160 acc_test: 0.6950 time: 0.1977s\n",
      "Epoch: 0013 loss_train: 1.9556 acc_train: 0.6500 acc_val: 0.7200 acc_test: 0.7100 time: 0.1974s\n",
      "Epoch: 0014 loss_train: 2.1249 acc_train: 0.8000 acc_val: 0.7220 acc_test: 0.7020 time: 0.1969s\n",
      "Epoch: 0015 loss_train: 1.4787 acc_train: 0.7333 acc_val: 0.7360 acc_test: 0.7370 time: 0.1964s\n",
      "Epoch: 0016 loss_train: 1.5631 acc_train: 0.7333 acc_val: 0.7460 acc_test: 0.7260 time: 0.1953s\n",
      "Epoch: 0017 loss_train: 1.8095 acc_train: 0.7000 acc_val: 0.7340 acc_test: 0.7250 time: 0.1973s\n",
      "Epoch: 0018 loss_train: 2.3107 acc_train: 0.7833 acc_val: 0.7180 acc_test: 0.7050 time: 0.1974s\n",
      "Epoch: 0019 loss_train: 2.0616 acc_train: 0.7500 acc_val: 0.7620 acc_test: 0.7470 time: 0.1966s\n",
      "Epoch: 0020 loss_train: 1.2335 acc_train: 0.7333 acc_val: 0.7700 acc_test: 0.7550 time: 0.1946s\n",
      "Epoch: 0021 loss_train: 1.7670 acc_train: 0.8667 acc_val: 0.7320 acc_test: 0.7340 time: 0.1963s\n",
      "Epoch: 0022 loss_train: 1.0288 acc_train: 0.7667 acc_val: 0.7580 acc_test: 0.7640 time: 0.1966s\n",
      "Epoch: 0023 loss_train: 1.7787 acc_train: 0.8500 acc_val: 0.7520 acc_test: 0.7580 time: 0.1983s\n",
      "Epoch: 0024 loss_train: 0.9244 acc_train: 0.8000 acc_val: 0.7380 acc_test: 0.7360 time: 0.1972s\n",
      "Epoch: 0025 loss_train: 0.7614 acc_train: 0.8667 acc_val: 0.7360 acc_test: 0.7430 time: 0.1963s\n",
      "Epoch: 0026 loss_train: 0.9824 acc_train: 0.8000 acc_val: 0.7820 acc_test: 0.7420 time: 0.1963s\n",
      "Epoch: 0027 loss_train: 0.8352 acc_train: 0.7833 acc_val: 0.7400 acc_test: 0.7370 time: 0.1976s\n",
      "Epoch: 0028 loss_train: 0.9301 acc_train: 0.8167 acc_val: 0.7300 acc_test: 0.7330 time: 0.1968s\n",
      "Epoch: 0029 loss_train: 0.5819 acc_train: 0.8833 acc_val: 0.7800 acc_test: 0.7710 time: 0.1969s\n",
      "Epoch: 0030 loss_train: 0.7439 acc_train: 0.8333 acc_val: 0.7760 acc_test: 0.7630 time: 0.1980s\n",
      "Epoch: 0031 loss_train: 0.7326 acc_train: 0.8000 acc_val: 0.7860 acc_test: 0.7710 time: 0.1987s\n",
      "Epoch: 0032 loss_train: 0.8255 acc_train: 0.8000 acc_val: 0.7540 acc_test: 0.7590 time: 0.1952s\n",
      "Epoch: 0033 loss_train: 0.8062 acc_train: 0.9000 acc_val: 0.7560 acc_test: 0.7580 time: 0.1967s\n",
      "Epoch: 0034 loss_train: 0.5956 acc_train: 0.9000 acc_val: 0.8000 acc_test: 0.7880 time: 0.1993s\n",
      "Epoch: 0035 loss_train: 0.5065 acc_train: 0.9167 acc_val: 0.7340 acc_test: 0.7360 time: 0.1959s\n",
      "Epoch: 0036 loss_train: 0.8265 acc_train: 0.8167 acc_val: 0.8100 acc_test: 0.7870 time: 0.1991s\n",
      "Epoch: 0037 loss_train: 0.7624 acc_train: 0.8500 acc_val: 0.6940 acc_test: 0.6750 time: 0.1960s\n",
      "Epoch: 0038 loss_train: 0.7372 acc_train: 0.8833 acc_val: 0.7240 acc_test: 0.7230 time: 0.1964s\n",
      "Epoch: 0039 loss_train: 0.5811 acc_train: 0.8333 acc_val: 0.7880 acc_test: 0.7820 time: 0.1963s\n",
      "Epoch: 0040 loss_train: 0.6857 acc_train: 0.8500 acc_val: 0.7600 acc_test: 0.7420 time: 0.1958s\n",
      "Epoch: 0041 loss_train: 0.7563 acc_train: 0.7833 acc_val: 0.8040 acc_test: 0.7940 time: 0.1976s\n",
      "Epoch: 0042 loss_train: 0.6602 acc_train: 0.9000 acc_val: 0.7680 acc_test: 0.7730 time: 0.1970s\n",
      "Epoch: 0043 loss_train: 0.6761 acc_train: 0.9167 acc_val: 0.7920 acc_test: 0.7810 time: 0.1983s\n",
      "Epoch: 0044 loss_train: 0.7632 acc_train: 0.9000 acc_val: 0.7680 acc_test: 0.7660 time: 0.1970s\n",
      "Epoch: 0045 loss_train: 0.5944 acc_train: 0.9167 acc_val: 0.7660 acc_test: 0.7590 time: 0.1975s\n",
      "Epoch: 0046 loss_train: 0.5493 acc_train: 0.9000 acc_val: 0.7920 acc_test: 0.7860 time: 0.1988s\n",
      "Epoch: 0047 loss_train: 0.6919 acc_train: 0.8333 acc_val: 0.7340 acc_test: 0.7170 time: 0.1951s\n",
      "Epoch: 0048 loss_train: 0.6376 acc_train: 0.8667 acc_val: 0.7920 acc_test: 0.7800 time: 0.1974s\n",
      "Epoch: 0049 loss_train: 0.7370 acc_train: 0.9333 acc_val: 0.7720 acc_test: 0.7750 time: 0.1976s\n",
      "Epoch: 0050 loss_train: 0.7753 acc_train: 0.9000 acc_val: 0.7740 acc_test: 0.7610 time: 0.1962s\n",
      "Epoch: 0051 loss_train: 0.4520 acc_train: 0.8667 acc_val: 0.7580 acc_test: 0.7530 time: 0.1962s\n",
      "Epoch: 0052 loss_train: 0.7007 acc_train: 0.9167 acc_val: 0.7800 acc_test: 0.7640 time: 0.1970s\n",
      "Epoch: 0053 loss_train: 0.4554 acc_train: 0.9500 acc_val: 0.7660 acc_test: 0.7850 time: 0.1974s\n",
      "Epoch: 0054 loss_train: 0.6271 acc_train: 0.8833 acc_val: 0.7680 acc_test: 0.7610 time: 0.1966s\n",
      "Epoch: 0055 loss_train: 0.4803 acc_train: 0.9000 acc_val: 0.7580 acc_test: 0.7510 time: 0.1973s\n",
      "Epoch: 0056 loss_train: 0.6274 acc_train: 0.8333 acc_val: 0.6420 acc_test: 0.6270 time: 0.1957s\n",
      "Epoch: 0057 loss_train: 0.6234 acc_train: 0.8167 acc_val: 0.7580 acc_test: 0.7620 time: 0.1973s\n",
      "Epoch: 0058 loss_train: 0.5822 acc_train: 0.8500 acc_val: 0.7140 acc_test: 0.6910 time: 0.1986s\n",
      "Epoch: 0059 loss_train: 0.8757 acc_train: 0.8167 acc_val: 0.7800 acc_test: 0.7760 time: 0.1970s\n",
      "Epoch: 0060 loss_train: 0.9240 acc_train: 0.8667 acc_val: 0.7840 acc_test: 0.7780 time: 0.1962s\n",
      "Epoch: 0061 loss_train: 0.7578 acc_train: 0.9000 acc_val: 0.7240 acc_test: 0.7080 time: 0.1975s\n",
      "Epoch: 0062 loss_train: 0.8856 acc_train: 0.8667 acc_val: 0.7820 acc_test: 0.7640 time: 0.1976s\n",
      "Epoch: 0063 loss_train: 0.6732 acc_train: 0.8167 acc_val: 0.8120 acc_test: 0.8030 time: 0.1981s\n",
      "Epoch: 0064 loss_train: 0.6730 acc_train: 0.9167 acc_val: 0.7700 acc_test: 0.7590 time: 0.1962s\n",
      "Epoch: 0065 loss_train: 0.7161 acc_train: 0.8167 acc_val: 0.7320 acc_test: 0.7250 time: 0.1959s\n",
      "Epoch: 0066 loss_train: 0.6684 acc_train: 0.8667 acc_val: 0.8000 acc_test: 0.7810 time: 0.1982s\n",
      "Epoch: 0067 loss_train: 0.7039 acc_train: 0.9000 acc_val: 0.7740 acc_test: 0.7480 time: 0.1963s\n",
      "Epoch: 0068 loss_train: 0.7072 acc_train: 0.8333 acc_val: 0.7440 acc_test: 0.7390 time: 0.1982s\n",
      "Epoch: 0069 loss_train: 0.5887 acc_train: 0.9000 acc_val: 0.7060 acc_test: 0.6970 time: 0.1971s\n",
      "Epoch: 0070 loss_train: 0.6448 acc_train: 0.8500 acc_val: 0.7940 acc_test: 0.8020 time: 0.1972s\n",
      "Epoch: 0071 loss_train: 0.4866 acc_train: 0.8833 acc_val: 0.7320 acc_test: 0.7290 time: 0.1983s\n",
      "Epoch: 0072 loss_train: 0.7152 acc_train: 0.7333 acc_val: 0.7860 acc_test: 0.7830 time: 0.1979s\n",
      "Epoch: 0073 loss_train: 0.4158 acc_train: 0.9167 acc_val: 0.7140 acc_test: 0.7120 time: 0.1982s\n",
      "Epoch: 0074 loss_train: 0.6190 acc_train: 0.8667 acc_val: 0.7640 acc_test: 0.7620 time: 0.1982s\n",
      "Epoch: 0075 loss_train: 0.4954 acc_train: 0.9000 acc_val: 0.8120 acc_test: 0.7940 time: 0.1982s\n",
      "Epoch: 0076 loss_train: 0.4646 acc_train: 0.9333 acc_val: 0.7760 acc_test: 0.7570 time: 0.1947s\n",
      "Epoch: 0077 loss_train: 0.7280 acc_train: 0.7833 acc_val: 0.7720 acc_test: 0.7710 time: 0.1966s\n",
      "Epoch: 0078 loss_train: 0.6760 acc_train: 0.9167 acc_val: 0.7840 acc_test: 0.7800 time: 0.1981s\n",
      "Epoch: 0079 loss_train: 0.5918 acc_train: 0.8167 acc_val: 0.8080 acc_test: 0.8010 time: 0.1977s\n",
      "Epoch: 0080 loss_train: 0.4785 acc_train: 0.9500 acc_val: 0.7740 acc_test: 0.7580 time: 0.1970s\n",
      "Epoch: 0081 loss_train: 0.6704 acc_train: 0.9000 acc_val: 0.7540 acc_test: 0.7450 time: 0.1981s\n",
      "Epoch: 0082 loss_train: 0.6544 acc_train: 0.8500 acc_val: 0.7780 acc_test: 0.7760 time: 0.1983s\n",
      "Epoch: 0083 loss_train: 0.7242 acc_train: 0.8833 acc_val: 0.7760 acc_test: 0.7760 time: 0.1995s\n",
      "Epoch: 0084 loss_train: 0.7121 acc_train: 0.8833 acc_val: 0.7800 acc_test: 0.7780 time: 0.1947s\n",
      "Epoch: 0085 loss_train: 0.5328 acc_train: 0.9000 acc_val: 0.7480 acc_test: 0.7290 time: 0.1967s\n",
      "Epoch: 0086 loss_train: 0.7568 acc_train: 0.8333 acc_val: 0.8040 acc_test: 0.7940 time: 0.1961s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0087 loss_train: 0.5302 acc_train: 0.8833 acc_val: 0.7940 acc_test: 0.7950 time: 0.1958s\n",
      "Epoch: 0088 loss_train: 0.5974 acc_train: 0.9167 acc_val: 0.7620 acc_test: 0.7710 time: 0.1962s\n",
      "Epoch: 0089 loss_train: 0.6073 acc_train: 0.8500 acc_val: 0.7940 acc_test: 0.7830 time: 0.1963s\n",
      "Epoch: 0090 loss_train: 0.5588 acc_train: 0.9167 acc_val: 0.7820 acc_test: 0.7630 time: 0.1968s\n",
      "Epoch: 0091 loss_train: 0.5610 acc_train: 0.9333 acc_val: 0.7920 acc_test: 0.7800 time: 0.1964s\n",
      "Epoch: 0092 loss_train: 0.5965 acc_train: 0.8667 acc_val: 0.7980 acc_test: 0.7860 time: 0.1972s\n",
      "Epoch: 0093 loss_train: 0.8312 acc_train: 0.9000 acc_val: 0.7660 acc_test: 0.7660 time: 0.1970s\n",
      "Epoch: 0094 loss_train: 0.5974 acc_train: 0.8500 acc_val: 0.7500 acc_test: 0.7430 time: 0.1960s\n",
      "Epoch: 0095 loss_train: 0.6925 acc_train: 0.8833 acc_val: 0.7840 acc_test: 0.7690 time: 0.1976s\n",
      "Epoch: 0096 loss_train: 0.5530 acc_train: 0.8667 acc_val: 0.8020 acc_test: 0.7800 time: 0.1968s\n",
      "Epoch: 0097 loss_train: 0.5796 acc_train: 0.8667 acc_val: 0.7960 acc_test: 0.7850 time: 0.1965s\n",
      "Epoch: 0098 loss_train: 0.4751 acc_train: 0.8833 acc_val: 0.7380 acc_test: 0.7280 time: 0.1971s\n",
      "Epoch: 0099 loss_train: 0.5567 acc_train: 0.8833 acc_val: 0.7640 acc_test: 0.7580 time: 0.1966s\n",
      "Epoch: 0100 loss_train: 0.5982 acc_train: 0.9167 acc_val: 0.7900 acc_test: 0.7850 time: 0.1959s\n",
      "Epoch: 0101 loss_train: 0.4992 acc_train: 0.8833 acc_val: 0.7960 acc_test: 0.7750 time: 0.1976s\n",
      "Epoch: 0102 loss_train: 0.5586 acc_train: 0.8833 acc_val: 0.7480 acc_test: 0.7370 time: 0.1981s\n",
      "Epoch: 0103 loss_train: 0.5575 acc_train: 0.8500 acc_val: 0.7580 acc_test: 0.7490 time: 0.1971s\n",
      "Epoch: 0104 loss_train: 0.7024 acc_train: 0.8833 acc_val: 0.8000 acc_test: 0.7930 time: 0.1979s\n",
      "Epoch: 0105 loss_train: 0.5127 acc_train: 0.8833 acc_val: 0.7760 acc_test: 0.7850 time: 0.1983s\n",
      "Epoch: 0106 loss_train: 0.5136 acc_train: 0.9000 acc_val: 0.7700 acc_test: 0.7540 time: 0.1952s\n",
      "Epoch: 0107 loss_train: 0.4753 acc_train: 0.9333 acc_val: 0.7820 acc_test: 0.7700 time: 0.1972s\n",
      "Epoch: 0108 loss_train: 0.5413 acc_train: 0.9000 acc_val: 0.8060 acc_test: 0.8000 time: 0.1976s\n",
      "Epoch: 0109 loss_train: 0.5735 acc_train: 0.9000 acc_val: 0.7980 acc_test: 0.7850 time: 0.1975s\n",
      "Epoch: 0110 loss_train: 0.5252 acc_train: 0.9000 acc_val: 0.7820 acc_test: 0.7660 time: 0.1964s\n",
      "Epoch: 0111 loss_train: 0.5163 acc_train: 0.9000 acc_val: 0.7920 acc_test: 0.7730 time: 0.1959s\n",
      "Epoch: 0112 loss_train: 0.4956 acc_train: 0.8667 acc_val: 0.8160 acc_test: 0.8070 time: 0.1962s\n",
      "Epoch: 0113 loss_train: 0.5548 acc_train: 0.8667 acc_val: 0.7840 acc_test: 0.7700 time: 0.1969s\n",
      "Epoch: 0114 loss_train: 0.4869 acc_train: 0.9333 acc_val: 0.8000 acc_test: 0.7910 time: 0.1977s\n",
      "Epoch: 0115 loss_train: 0.4460 acc_train: 0.9167 acc_val: 0.8140 acc_test: 0.8000 time: 0.1963s\n",
      "Epoch: 0116 loss_train: 0.6178 acc_train: 0.9167 acc_val: 0.8140 acc_test: 0.8050 time: 0.1983s\n",
      "Epoch: 0117 loss_train: 0.5591 acc_train: 0.8667 acc_val: 0.7780 acc_test: 0.7690 time: 0.1975s\n",
      "Epoch: 0118 loss_train: 0.5654 acc_train: 0.8500 acc_val: 0.8180 acc_test: 0.8030 time: 0.1976s\n",
      "Epoch: 0119 loss_train: 0.5243 acc_train: 0.8833 acc_val: 0.7920 acc_test: 0.7750 time: 0.1974s\n",
      "Epoch: 0120 loss_train: 0.5205 acc_train: 0.9333 acc_val: 0.7580 acc_test: 0.7380 time: 0.1960s\n",
      "Epoch: 0121 loss_train: 0.4573 acc_train: 0.9167 acc_val: 0.7940 acc_test: 0.7860 time: 0.1956s\n",
      "Epoch: 0122 loss_train: 0.4686 acc_train: 0.9000 acc_val: 0.8040 acc_test: 0.7940 time: 0.1957s\n",
      "Epoch: 0123 loss_train: 0.4462 acc_train: 0.9167 acc_val: 0.8120 acc_test: 0.8030 time: 0.1957s\n",
      "Epoch: 0124 loss_train: 0.4284 acc_train: 0.9167 acc_val: 0.8200 acc_test: 0.8030 time: 0.1963s\n",
      "Epoch: 0125 loss_train: 0.4942 acc_train: 0.8833 acc_val: 0.7940 acc_test: 0.7880 time: 0.1962s\n",
      "Epoch: 0126 loss_train: 0.4631 acc_train: 0.8833 acc_val: 0.8140 acc_test: 0.8060 time: 0.1974s\n",
      "Epoch: 0127 loss_train: 0.3669 acc_train: 0.9167 acc_val: 0.7900 acc_test: 0.7810 time: 0.1971s\n",
      "Epoch: 0128 loss_train: 0.4494 acc_train: 0.9000 acc_val: 0.8160 acc_test: 0.8010 time: 0.1957s\n",
      "Epoch: 0129 loss_train: 0.4393 acc_train: 0.8500 acc_val: 0.8100 acc_test: 0.8130 time: 0.1983s\n",
      "Epoch: 0130 loss_train: 0.5075 acc_train: 0.8500 acc_val: 0.7940 acc_test: 0.7810 time: 0.1957s\n",
      "Epoch: 0131 loss_train: 0.4849 acc_train: 0.9167 acc_val: 0.7840 acc_test: 0.7790 time: 0.1962s\n",
      "Epoch: 0132 loss_train: 0.4920 acc_train: 0.9167 acc_val: 0.7980 acc_test: 0.7880 time: 0.1982s\n",
      "Epoch: 0133 loss_train: 0.4501 acc_train: 0.9167 acc_val: 0.7920 acc_test: 0.7810 time: 0.1969s\n",
      "Epoch: 0134 loss_train: 0.4771 acc_train: 0.9167 acc_val: 0.7960 acc_test: 0.7930 time: 0.1967s\n",
      "Epoch: 0135 loss_train: 0.4637 acc_train: 0.9167 acc_val: 0.7600 acc_test: 0.7430 time: 0.1963s\n",
      "Epoch: 0136 loss_train: 0.4915 acc_train: 0.9000 acc_val: 0.7900 acc_test: 0.7900 time: 0.1968s\n",
      "Epoch: 0137 loss_train: 0.4311 acc_train: 0.9000 acc_val: 0.7960 acc_test: 0.7970 time: 0.1965s\n",
      "Epoch: 0138 loss_train: 0.4515 acc_train: 0.9167 acc_val: 0.7980 acc_test: 0.7900 time: 0.1965s\n",
      "Epoch: 0139 loss_train: 0.5101 acc_train: 0.8833 acc_val: 0.7540 acc_test: 0.7350 time: 0.1979s\n",
      "Epoch: 0140 loss_train: 0.4471 acc_train: 0.9000 acc_val: 0.7960 acc_test: 0.7910 time: 0.1975s\n",
      "Epoch: 0141 loss_train: 0.4555 acc_train: 0.9167 acc_val: 0.8160 acc_test: 0.8050 time: 0.1990s\n",
      "Epoch: 0142 loss_train: 0.4049 acc_train: 0.8833 acc_val: 0.7720 acc_test: 0.7640 time: 0.1964s\n",
      "Epoch: 0143 loss_train: 0.4583 acc_train: 0.9333 acc_val: 0.7860 acc_test: 0.7810 time: 0.1969s\n",
      "Epoch: 0144 loss_train: 0.3205 acc_train: 0.9333 acc_val: 0.8020 acc_test: 0.7930 time: 0.1983s\n",
      "Epoch: 0145 loss_train: 0.4238 acc_train: 0.9000 acc_val: 0.7800 acc_test: 0.7810 time: 0.1961s\n",
      "Epoch: 0146 loss_train: 0.4691 acc_train: 0.8667 acc_val: 0.7740 acc_test: 0.7680 time: 0.1968s\n",
      "Epoch: 0147 loss_train: 0.4094 acc_train: 0.9000 acc_val: 0.8060 acc_test: 0.8010 time: 0.1954s\n",
      "Epoch: 0148 loss_train: 0.4480 acc_train: 0.8833 acc_val: 0.8100 acc_test: 0.8030 time: 0.1967s\n",
      "Epoch: 0149 loss_train: 0.4008 acc_train: 0.9167 acc_val: 0.8060 acc_test: 0.7940 time: 0.1953s\n",
      "Epoch: 0150 loss_train: 0.5162 acc_train: 0.8500 acc_val: 0.7540 acc_test: 0.7370 time: 0.1979s\n",
      "Epoch: 0151 loss_train: 0.4292 acc_train: 0.9167 acc_val: 0.7740 acc_test: 0.7670 time: 0.1982s\n",
      "Epoch: 0152 loss_train: 0.4156 acc_train: 0.8667 acc_val: 0.7200 acc_test: 0.7070 time: 0.1973s\n",
      "Epoch: 0153 loss_train: 0.4738 acc_train: 0.8667 acc_val: 0.7820 acc_test: 0.7680 time: 0.1982s\n",
      "Epoch: 0154 loss_train: 0.3312 acc_train: 0.9500 acc_val: 0.7460 acc_test: 0.7220 time: 0.1982s\n",
      "Epoch: 0155 loss_train: 0.4526 acc_train: 0.8500 acc_val: 0.8020 acc_test: 0.7920 time: 0.1994s\n",
      "Epoch: 0156 loss_train: 0.4426 acc_train: 0.9167 acc_val: 0.7980 acc_test: 0.7970 time: 0.1962s\n",
      "Epoch: 0157 loss_train: 0.4472 acc_train: 0.8833 acc_val: 0.7680 acc_test: 0.7540 time: 0.1976s\n",
      "Epoch: 0158 loss_train: 0.4925 acc_train: 0.8667 acc_val: 0.8000 acc_test: 0.7910 time: 0.1970s\n",
      "Epoch: 0159 loss_train: 0.4116 acc_train: 0.9167 acc_val: 0.8020 acc_test: 0.7960 time: 0.1972s\n",
      "Epoch: 0160 loss_train: 0.3819 acc_train: 0.8833 acc_val: 0.7800 acc_test: 0.7700 time: 0.1971s\n",
      "Epoch: 0161 loss_train: 0.4074 acc_train: 0.9167 acc_val: 0.7900 acc_test: 0.7770 time: 0.1969s\n",
      "Epoch: 0162 loss_train: 0.3746 acc_train: 0.9000 acc_val: 0.8100 acc_test: 0.7940 time: 0.1978s\n",
      "Epoch: 0163 loss_train: 0.3763 acc_train: 0.9333 acc_val: 0.7940 acc_test: 0.7780 time: 0.1976s\n",
      "Epoch: 0164 loss_train: 0.3584 acc_train: 0.9000 acc_val: 0.7880 acc_test: 0.7800 time: 0.1960s\n",
      "Epoch: 0165 loss_train: 0.4051 acc_train: 0.9333 acc_val: 0.7940 acc_test: 0.7770 time: 0.1952s\n",
      "Epoch: 0166 loss_train: 0.3976 acc_train: 0.9500 acc_val: 0.8060 acc_test: 0.7860 time: 0.1958s\n",
      "Epoch: 0167 loss_train: 0.4428 acc_train: 0.9000 acc_val: 0.7720 acc_test: 0.7790 time: 0.1959s\n",
      "Epoch: 0168 loss_train: 0.4275 acc_train: 0.9167 acc_val: 0.7820 acc_test: 0.7740 time: 0.1975s\n",
      "Epoch: 0169 loss_train: 0.4643 acc_train: 0.9000 acc_val: 0.7960 acc_test: 0.7840 time: 0.1977s\n",
      "Epoch: 0170 loss_train: 0.3628 acc_train: 0.9333 acc_val: 0.7840 acc_test: 0.7670 time: 0.1963s\n",
      "Epoch: 0171 loss_train: 0.3657 acc_train: 0.9333 acc_val: 0.7860 acc_test: 0.7820 time: 0.1961s\n",
      "Epoch: 0172 loss_train: 0.3260 acc_train: 0.9333 acc_val: 0.7960 acc_test: 0.7890 time: 0.1962s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0173 loss_train: 0.3586 acc_train: 0.9333 acc_val: 0.7860 acc_test: 0.7850 time: 0.1964s\n",
      "Epoch: 0174 loss_train: 0.3507 acc_train: 0.8833 acc_val: 0.7880 acc_test: 0.7700 time: 0.1982s\n",
      "Epoch: 0175 loss_train: 0.4069 acc_train: 0.9000 acc_val: 0.7580 acc_test: 0.7360 time: 0.1976s\n",
      "Epoch: 0176 loss_train: 0.3919 acc_train: 0.8667 acc_val: 0.8020 acc_test: 0.7930 time: 0.1978s\n",
      "Epoch: 0177 loss_train: 0.3977 acc_train: 0.8667 acc_val: 0.7940 acc_test: 0.7860 time: 0.1978s\n",
      "Epoch: 0178 loss_train: 0.3764 acc_train: 0.9000 acc_val: 0.7840 acc_test: 0.7740 time: 0.1965s\n",
      "Epoch: 0179 loss_train: 0.3120 acc_train: 0.9500 acc_val: 0.7560 acc_test: 0.7450 time: 0.1960s\n",
      "Epoch: 0180 loss_train: 0.4450 acc_train: 0.8667 acc_val: 0.8200 acc_test: 0.8070 time: 0.1976s\n",
      "Epoch: 0181 loss_train: 0.3389 acc_train: 0.9333 acc_val: 0.8040 acc_test: 0.7990 time: 0.1982s\n",
      "Epoch: 0182 loss_train: 0.4093 acc_train: 0.8833 acc_val: 0.7640 acc_test: 0.7560 time: 0.1968s\n",
      "Epoch: 0183 loss_train: 0.3324 acc_train: 0.9333 acc_val: 0.7780 acc_test: 0.7640 time: 0.1966s\n",
      "Epoch: 0184 loss_train: 0.4388 acc_train: 0.8667 acc_val: 0.7860 acc_test: 0.7810 time: 0.1970s\n",
      "Epoch: 0185 loss_train: 0.3107 acc_train: 0.9167 acc_val: 0.8120 acc_test: 0.7890 time: 0.1962s\n",
      "Epoch: 0186 loss_train: 0.3325 acc_train: 0.9167 acc_val: 0.7920 acc_test: 0.7900 time: 0.1972s\n",
      "Epoch: 0187 loss_train: 0.3238 acc_train: 0.8667 acc_val: 0.7820 acc_test: 0.7650 time: 0.1964s\n",
      "Epoch: 0188 loss_train: 0.3143 acc_train: 0.9000 acc_val: 0.7600 acc_test: 0.7520 time: 0.1962s\n",
      "Epoch: 0189 loss_train: 0.3196 acc_train: 0.9000 acc_val: 0.7820 acc_test: 0.7950 time: 0.1951s\n",
      "Epoch: 0190 loss_train: 0.4334 acc_train: 0.8833 acc_val: 0.8020 acc_test: 0.7970 time: 0.1958s\n",
      "Epoch: 0191 loss_train: 0.2684 acc_train: 0.9500 acc_val: 0.7800 acc_test: 0.7620 time: 0.1976s\n",
      "Epoch: 0192 loss_train: 0.2954 acc_train: 0.9000 acc_val: 0.7560 acc_test: 0.7330 time: 0.1968s\n",
      "Epoch: 0193 loss_train: 0.4019 acc_train: 0.8167 acc_val: 0.7980 acc_test: 0.7930 time: 0.1964s\n",
      "Epoch: 0194 loss_train: 0.3013 acc_train: 0.9167 acc_val: 0.7820 acc_test: 0.7830 time: 0.1968s\n",
      "Epoch: 0195 loss_train: 0.3218 acc_train: 0.9333 acc_val: 0.7760 acc_test: 0.7610 time: 0.1975s\n",
      "Epoch: 0196 loss_train: 0.3427 acc_train: 0.9167 acc_val: 0.7860 acc_test: 0.7580 time: 0.1961s\n",
      "Epoch: 0197 loss_train: 0.2962 acc_train: 0.9167 acc_val: 0.7880 acc_test: 0.7860 time: 0.1953s\n",
      "Epoch: 0198 loss_train: 0.3883 acc_train: 0.8833 acc_val: 0.7940 acc_test: 0.7880 time: 0.1966s\n",
      "Epoch: 0199 loss_train: 0.3603 acc_train: 0.8667 acc_val: 0.7540 acc_test: 0.7330 time: 0.1950s\n",
      "Epoch: 0200 loss_train: 0.3881 acc_train: 0.8500 acc_val: 0.7840 acc_test: 0.7720 time: 0.1957s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 39.8988s\n",
      "0.8200000000000001 0.807\n",
      "0.8180000000000001 0.803\n",
      "0.8160000000000001 0.805\n",
      "0.8140000000000001 0.806\n",
      "0.812 0.789\n",
      "0.81 0.794\n",
      "0.808 0.801\n",
      "0.806 0.786\n",
      "0.804 0.799\n",
      "0.802 0.797\n"
     ]
    }
   ],
   "source": [
    "model = GMLP(nfeat=features.shape[1],nhid =128,\n",
    "        nclass=labels.max().item() + 1,\n",
    "        dropout=0.3)\n",
    "model.cuda()\n",
    "optimizer = optim.Adam(model.parameters(),lr=0.1, weight_decay=5e-4)\n",
    "\n",
    "t_total = time.time()\n",
    "record = {}\n",
    "for epoch in range(epoches):\n",
    "    train(epoch,model,record,feature_list)\n",
    "        \n",
    "        \n",
    "print(\"Optimization Finished!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "bit_list = sorted(record.keys())\n",
    "bit_list.reverse()\n",
    "for key in bit_list[:10]:\n",
    "    value = record[key]\n",
    "    print(key,value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Even with 50 layers, GMLP still keeps a good performance!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
